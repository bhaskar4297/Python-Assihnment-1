Q 1 :Will the reducer work or not if you use “Limit 1” in any HiveQL query?
Ans :Yes, the reducer will work even if you use "LIMIT 1" in the HiveQL query. The reducer is responsible for aggregating the data from the mappers and producing the final output, and it will be executed regardless of whether a limit is specified or not.

Q 2 :Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time?
Ans :If multiple clients try to access Hive at the same time, the metastore (which stores metadata about tables, partitions, and databases) can become a bottleneck. By default, Hive uses a Derby database for the metastore, which is not designed for concurrent access.
As a result, the clients may experience slow response times or errors. To avoid this issue, it is recommended to use a shared metastore (such as MySQL or PostgreSQL) that can handle concurrent access.

Q 3 :Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ; 
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?
Ans :To improve the performance of the query, you can use partitioning to divide the data into smaller chunks based on the month. This will allow Hive to process only the relevant data for each month, instead of scanning the entire table. The steps to do this are:
Alter the table to add a partition column for month: ALTER TABLE transaction_details ADD PARTITION (month='Jan') ...
Load the data into the table with the partition key: LOAD DATA INPATH 'hdfs://path/to/data' OVERWRITE INTO TABLE transaction_details PARTITION (month='Jan') ...
Repeat the above steps for each month.
Run the query with the partition key: SELECT month, SUM(amount) FROM transaction_details GROUP BY month;


Q 4 :How can you add a new partition for the month December in the above partitioned table?
Ans :To add a new partition for the month of December, you can use the ALTER TABLE command:
ALTER TABLE transaction_details ADD PARTITION (month='Dec') 
LOCATION 'hdfs://path/to/december/data';

Q 5 :I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?
Ans :This error occurs when you are using dynamic partitioning and have not specified any static partition columns. To resolve the issue, you need to specify at least one static partition column using the SET command before inserting the data. For example:

SET hive.exec.dynamic.partition.mode=nonstrict;
INSERT INTO TABLE my_table PARTITION (date) VALUES (1, 'foo', 'bar', '2023-03-01');

Q 6 :Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries: id first_name last_name email gender ip_address How will you consume this CSV file into the Hive warehouse using built-in SerDe?
Ans :To consume the 'sample.csv' file into Hive warehouse using built-in SerDe, we can use the following steps:
a. Create a table in Hive with appropriate column names and data types matching the CSV file:
CREATE TABLE sample_table (
id INT,
first_name STRING,
last_name STRING,
email STRING,
gender STRING,
ip_address STRING) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

b. Load the CSV file into the table using the LOAD DATA command:
LOAD DATA LOCAL INPATH '/temp/sample.csv' OVERWRITE INTO TABLE sample_table;

c. Now the data from the CSV file should be available in the sample_table in the Hive warehouse.

Q 7 :Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}.
Now, as we know, Hadoop performance degrades when we use lots of small files.So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?
Ans :When we have a lot of small CSV files in the input directory in HDFS and we want to create a single Hive table for these files without degrading the performance of the system, we can use the following steps:
a. Combine the small files using the Hadoop FileUtil class:
hadoop fs -cat /input/* | hadoop fs -put - /tmp/bigfile.csv

b. Create a table in Hive with appropriate column names and data types matching the CSV file:
CREATE TABLE big_table (
id INT,
name STRING,
email STRING,
country STRING) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ',' 
STORED AS TEXTFILE;

c. Load the combined file into the table using the LOAD DATA command:
LOAD DATA LOCAL INPATH '/tmp/bigfile.csv' OVERWRITE INTO TABLE big_table;

d. Now the data from all the small files should be available in the big_table in the Hive warehouse.

Q 8 :LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;

The following statement failed to execute. What can be the cause?

Ans :The statement "LOAD DATA LOCAL INPATH ‘Home/country/state/’ OVERWRITE INTO TABLE address;" failed to execute because the path is incorrect. It should be an absolute HDFS path and not a local file system path. Also, the trailing '/' should be removed.

Q 9 :Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?
Ans :Yes, it is possible to add 100 nodes when we already have 100 nodes in Hive. This can be done by adding more nodes to the Hadoop cluster on which Hive is running. Once the new nodes are added, they will automatically be available to Hive for processing data. 
However, it is important to note that simply adding more nodes may not necessarily result in better performance, as the data distribution and processing algorithms also play a role. It is recommended to consult with a Hadoop expert or performance engineer to ensure 
that adding more nodes is the right solution for the performance issue.


Hive Practical questions:

Q : Hive Join operations

Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
)

Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)



ANS :
-- Create the CUSTOMERS table
CREATE TABLE CUSTOMERS (
  ID INT,
  NAME STRING,
  AGE INT,
  ADDRESS STRING,
  SALARY FLOAT
);

-- Create the ORDERS table
CREATE TABLE ORDERS (
  OID INT,
  DATE STRING,
  CUSTOMER_ID INT,
  AMOUNT FLOAT
);

-- Insert sample data into the CUSTOMERS table
INSERT INTO CUSTOMERS VALUES
  (1, 'bhaskar sinha', 25, 'durg', 50000),
  (2, 'bhavesh nayak', 30, 'bhilai', 60000),
  (3, 'akash sahu', 35, 'raipur', 70000),
  (4, 'vinamra bhangale', 40, 'bilaspur', 80000);

-- Insert sample data into the ORDERS table
INSERT INTO ORDERS VALUES
  (1001, '2022-01-01', 1, 1000),
  (1002, '2022-01-02', 2, 2000),
  (1003, '2022-01-03', 1, 3000),
  (1004, '2022-01-04', 3, 4000),
  (1005, '2022-01-05', 4, 5000);

-- Perform an inner join on the CUSTOMERS and ORDERS tables using the CUSTOMER_ID column
SELECT c.NAME, o.DATE, o.AMOUNT
FROM CUSTOMERS c
JOIN ORDERS o
ON c.ID = o.CUSTOMER_ID;

-- Perform a left outer join on the CUSTOMERS and ORDERS tables using the CUSTOMER_ID column
SELECT c.NAME, o.DATE, o.AMOUNT
FROM CUSTOMERS c
LEFT OUTER JOIN ORDERS o
ON c.ID = o.CUSTOMER_ID;

-- Perform a right outer join on the CUSTOMERS and ORDERS tables using the CUSTOMER_ID column
SELECT c.NAME, o.DATE, o.AMOUNT
FROM CUSTOMERS c
RIGHT OUTER JOIN ORDERS o
ON c.ID = o.CUSTOMER_ID;

-- Perform a full outer join on the CUSTOMERS and ORDERS tables using the CUSTOMER_ID column
SELECT c.NAME, o.DATE, o.AMOUNT
FROM CUSTOMERS c
FULL OUTER JOIN ORDERS o
ON c.ID = o.CUSTOMER_ID;

Q : BUILD A DATA PIPELINE WITH HIVE
ANS :
-- Once we have the external tables set up, we can then create an internal table that will be used to store the output of the data pipeline. 
This table will have a similar schema to the external tables, but it will be created in a different location.
CREATE TABLE customer_orders(
    id INT,
    name STRING,
    age INT,
    address STRING,
    salary FLOAT,
    oid INT,
    date STRING,
    customer_id INT,
    amount FLOAT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

-- We can then write a HiveQL query that joins the external tables and inserts the result into the internal table.

INSERT INTO TABLE customer_orders
SELECT c.id, c.name, c.age, c.address, c.salary, o.oid, o.date, o.customer_id, o.amount
FROM customers c
FULL OUTER JOIN orders o
ON c.id = o.customer_id;
Schedule the pipeline to run automatically using Oozie or any other scheduler.




Q : Download a data from the given location - 
ANS :
1. Create a hive table as per given schema in your dataset 
CREATE TABLE airquality (
  Date STRING,
  Time STRING,
  CO(GT) FLOAT,
  PT08.S1(CO) FLOAT,
  NMHC(GT) FLOAT
  C6H6(GT) FLOAT,
  PT08.S2(NMHC) FLOAT,
  NOx(GT) FLOAT,
  PT08.S3(NOx) FLOAT,
  NO2(GT) FLOAT,
  PT08.S4(NO2) FLOAT,
  PT08.S5(O3) FLOAT,
  T FLOAT,
  RH FLOAT,
  AH FLOAT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

2. try to place a data into table location
LOAD DATA INPATH '/user/hive/airquality_data.csv' INTO TABLE airquality;

3. Perform a select operation .
SELECT * FROM airquality;

4. Fetch the result of the select operation in your local as a csv file .
INSERT OVERWRITE LOCAL DIRECTORY '/path/to/local_directory'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
SELECT * FROM airquality;

5. Perform group by operation 
SELECT Date, AVG(CO(GT)) AS avg_CO FROM airquality GROUP BY Date;

7. Perform five different filter operations such as =, !=, >, <, BETWEEN.

8. show and example of regex operation.
SELECT * FROM airquality WHERE Time RLIKE '^[0-9]{2}:00:00$';

9. alter table operation 
ALTER TABLE airquality ADD COLUMNS (Year INT);
This command will add a new column named Year to the airquality table.

10 . drop table operation
DROP TABLE airquality;
This command will delete the airquality table and its data.

12 . order by operation .
SELECT * FROM airquality ORDER BY CO(GT) DESC;
This query will return all records in the airquality table sorted by the CO column in descending order.

13 . where clause operations you have to perform 
=, !=, >, <, >=, <=, BETWEEN, LIKE, RLIKE.

14 . sorting operation you have to perform .
SELECT * FROM airquality SORT BY CO(GT) DESC;
This query will return all records in the airquality table sorted by the CO(GT) column in descending order.

15 . distinct operation you have to perform .
SELECT DISTINCT Country FROM airquality;
This query will return a list of all distinct values in the Country column of the airquality table.

16 . like an operation you have to perform .
SELECT * FROM airquality WHERE Country LIKE 'U%';
This query will return all records in the airquality table where the Country column starts with the letter U.

17 . union operation you have to perform .
SELECT Date, CO FROM airquality WHERE CO(GT) > 1.0
UNION
SELECT Date, C6H6 FROM airquality WHERE C6H6(GT) > 1.0;
This query will return all records where either the CO column or the C6H6 column is greater than 1.0 and combine the results into a single table.

18 . table view operation you have to perform .
CREATE VIEW high_CO_data AS
SELECT * FROM airquality WHERE CO(GT) > 1.0;
This command will create a view named high_CO_data that only includes records where the CO column is greater than 1.0. You can then query this view like a regular table.


Q : hive operation with python
ANS :
from pyhive import hive
import pandas as pd

# Connect to Hive
conn = hive.Connection(host='localhost', port=10000, auth='NOSASL')

# Create a cursor object
cursor = conn.cursor()

# Create a table as per the given schema
cursor.execute("CREATE TABLE airquality (Date STRING, Time STRING, CO FLOAT, PT08S1 FLOAT, NMHC FLOAT, C6H6 FLOAT, PT08S2 FLOAT, NOX FLOAT, PT08S3 FLOAT, NO2 FLOAT, PT08S4 FLOAT, PT08S5 FLOAT, T FLOAT, RH FLOAT, AH FLOAT, Country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE")

# Load data into table
cursor.execute("LOAD DATA INPATH '/path/to/data.csv' INTO TABLE airquality")

# Select data from table
cursor.execute("SELECT * FROM airquality LIMIT 10")
result = cursor.fetchall()
df = pd.DataFrame(result, columns=[desc[0] for desc in cursor.description])
print(df)

# Group by operation
cursor.execute("SELECT Country, AVG(CO) FROM airquality GROUP BY Country")
result = cursor.fetchall()
df = pd.DataFrame(result, columns=[desc[0] for desc in cursor.description])
print(df)

# Filter operations
cursor.execute("SELECT * FROM airquality WHERE Country = 'Canada'")
result = cursor.fetchall()
df = pd.DataFrame(result, columns=[desc[0] for desc in cursor.description])
print(df)

cursor.execute("SELECT * FROM airquality WHERE C6H6 > 5.0")
result = cursor.fetchall()
df = pd.DataFrame(result, columns=[desc[0] for desc in cursor.description])
print(df)

# Sorting operation
cursor.execute("SELECT * FROM airquality ORDER BY CO DESC")
result = cursor.fetchall()
df = pd.DataFrame(result, columns=[desc[0] for desc in cursor.description])
print(df)

# Distinct operation
cursor.execute("SELECT DISTINCT Country FROM airquality")
result = cursor.fetchall()
df = pd.DataFrame(result, columns=[desc[0] for desc in cursor.description])
print(df)

# Like operation
cursor.execute("SELECT * FROM airquality WHERE Country LIKE 'U%'")
result = cursor.fetchall()
df = pd.DataFrame(result, columns=[desc[0] for desc in cursor.description])
print(df)

# Union operation
cursor.execute("SELECT Date, CO FROM airquality WHERE CO > 1.0 \
                UNION \
                SELECT Date, C6H6 FROM airquality WHERE C6H6 > 1.0")
result = cursor.fetchall()
df = pd.DataFrame(result, columns=[desc[0] for desc in cursor.description])
print(df)

# Close connection
conn.close()

Q : Create a python application that connects to the Hive database for extracting data, creating sub tables for data processing, drops temporary tables.
fetch rows to python itself into a list of tuples and mimic the join or filter operations.
ANS :
from pyhive import hive
import pandas as pd

# create a connection to the Hive database
conn = hive.Connection(host='localhost', port=10000, username='your_username')

# create a cursor object to execute queries
cursor = conn.cursor()

# execute a query to extract data from a table
cursor.execute('SELECT * FROM airquality')

# create a sub-table for data processing
cursor.execute('CREATE TABLE temp_table AS SELECT Date, CO, C6H6 FROM airquality WHERE CO > 1.0')

# drop the temporary table
cursor.execute('DROP TABLE temp_table')

# fetch rows from the table to Python itself into a list of tuples
rows = cursor.fetchall()

# mimic join operation
df1 = pd.DataFrame(rows, columns=['Date', 'CO', 'C6H6'])
df2 = pd.DataFrame(rows, columns=['Date', 'NO2', 'SO2'])
df3 = pd.merge(df1, df2, on='Date')

# mimic filter operation
df_filtered = df3[df3['CO'] > 1.0]

# close the cursor and connection
cursor.close()
conn.close()

